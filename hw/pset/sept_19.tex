\documentclass[12pt,letterpaper,fleqn]{hmcpset}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{parskip}

\input{../../materials/section/macros.tex}

% info for header block in upper right hand corner
\name{------}
\class{Math 189r}
\assignment{Homework 1}
\duedate{September 19, 2016}

\begin{document}

There are 8 problems in this set. 4 of the problems (you choose except this first
set must include problem 1 and/or 2) are due on September 12,
and the rest of the problems are due on September 19. Feel
free to work with other students, but make sure you write up the homework
and code on your own (no copying homework \textit{or} code; no pair programming).
Feel free to ask students or instructors for help debugging code or whatever else,
though.
When implementing algorithms you may not use any library (such as \texttt{sklearn})
that already implements the algorithms but you may use any other library for
data cleaning and numeric purposes (\texttt{numpy} or \texttt{pandas}). Use common
sense. Problems are in no specific order.\\[1em]


\textbf{1} (\textbf{regression}). Download the data at 
\url{https://math189r.github.io/hw/data/online_news_popularity/online_news_popularity.csv}
and the info file at
\url{https://math189r.github.io/hw/data/online_news_popularity/online_news_popularity.txt}.
Read the info file. Split the csv file into a training and test set with
the first two thirds of the data in the training set and the rest for testing.
Of the testing data, split the first half into a `validation set' (used
to optimize hyperparameters while leaving your testing data pristine) and
the remaining half as your test set.
We will use this data for the remainder of the problem. The goal of this data
is to predict the \textbf{log} number of shares a news article will have given the other
features.
\begin{enumerate}[(a)]
    \item (\textbf{math}) Show that the maximum a posteriori problem for
        linear regression with a zero-mean Gaussian prior $\PP(\ww) = \prod_j
        \Nc(w_j | 0, \tau^2)$ on the weights,
        \[
            \argmax_\ww \sum_{i=1}^N \log\Nc(y_i | w_0 + \ww^\T\xx_i, \sigma^2) + \sum_{j=1}^D \log\Nc(w_j | 0, \tau^2)
        \]
        is equivalent to the ridge regression problem
        \[
            \argmin \frac{1}{N}\sum_{i=1}^N (y_i - (w_0 + \ww^\T\xx_i))^2 + \lambda ||\ww||_2^2
        \]
        with $\lambda = \sigma^2 / \tau^2$.
    \item (\textbf{math}) Find a closed form solution $\xx^\star$ to the ridge regression
        problem:
        \[
            \text{minimize: } ||A\xx - \bb||_2^2 + ||\Gamma\xx||_2^2.
        \]
    \item (\textbf{implementation}) Attempt to predict the $\log\text{shares}$ using ridge
        regression from the previous problem solution. Make sure you include a bias
        term and \textit{don't regularize the bias term}.
        Find the optimal regularization parameter $\lambda$
        from the validation set. Plot both $\lambda$ versus the validation RMSE (you should have
        tried at least 150 parameter settings randomly chosen between 0.0 and 150.0 because
        the dataset is small)
        and $\lambda$ versus $||\thetab^\star||_2$ where $\thetab$ is your weight vector.
        What is the final RMSE on the test set with the optimal $\lambda^\star$?
    \item (\textbf{math}) Consider regularized linear regression where we pull the
        basis term out of the feature vectors. That is, instead of computing $\hat\yy
        = \thetab^\T\xx$ with $\xx_0 = 1$, we compute $\hat\yy = \thetab^\T\xx + b$.
        This corresponds to solving the optimization problem
        \[
            \text{minimize: } ||A\xx + b\1 - \yy||_2^2 + ||\Gamma\xx||_2^2.
        \]
        Solve for the optimal $\xx^\star$ explicitly. Use this close form to compute the
        bias term for the previous problem (with the same regularization strategy). Make
        sure it is the same.
    \item (\textbf{implementation}) We can also compute the solution to the least squares
        problem using gradient descent. Consider the same bias-relocated objective
        \[
            \text{minimize: } f = ||A\xx + b\1 - \yy||_2^2 + ||\Gamma\xx||_2^2.
        \]
        Compute the gradients and run gradient descent. Plot the $\ell_2$ norm
        between the optimal $(\xx^\star, b^\star)$ vector you computed in closed form
        and the iterates generated by gradient descent. Hint: your plot should move
        down and to the left and approach zero as the number of iterations increases. If
        it doesn't, try decreasing the learning rate.
\end{enumerate}

\textbf{2} (\textbf{MNIST}) Download the training set at 
\url{http://pjreddie.com/media/files/mnist_train.csv} and test set at
\url{http://pjreddie.com/media/files/mnist_test.csv}. This dataset, the MNIST
dataset, is a classic in the deep learning literature as a toy dataset to test
algorithms on. The problem is this: we have $28\times 28$ images of handwritten
digits as well as the label of which digit $0 \leq \texttt{label} \leq 9$ the written
digit corresponds to. Given a new image of a handwritten digit, we want to be
able to predict which digit it is.
The format of the data is \texttt{label, pix-11, pix-12, pix-13, ...}
where \texttt{pix-ij} is the pixel in the \texttt{ith} row and \texttt{jth} column.
\begin{enumerate}[(a)]
    \item (\textbf{logistic}) Restrict the dataset to only the digits with a label
        of 0 or 1. Implement L2 regularized logistic regression as a model to compute
        $\PP(y=1|\xx)$ for a different value of the regularization parameter $\lambda$.
        Plot the learning curve (objective vs. iteration) when using Newton's Method
        \textit{and} gradient descent.
        Plot the accuracy, precision ($p = \PP(y=1 | \hat y=1)$), recall ($r = \PP(\hat y=1 | y=1)$),
        and F1-score ($F1 = 2pr / (p+r)$) for different values of $\lambda$ (try at least
        10 different values including $\lambda = 0$) on the test set and report the
        value of $\lambda$ which maximizes the accuracy on the test set. What is your
        accuracy on the test set for this model? Your accuracy should definitely be
        over 90\%.

    \item (\textbf{softmax}) Now we will use the whole dataset and predict the label
        of each digit using L2 regularized softmax regression (multinomial logistic
        regression). Implement this using gradient descent, and plot the accuracy
        on the test set for different values of $\lambda$, the regularization parameter.
        Report the test accuracy for the optimal value of $\lambda$ as well as it's
        learning curve. Your accuracy should be over 90\%.

    \item (\textbf{KNN}) Solve the same problem posed in part (b) but use
        K-Nearest Neighbors instead of softmax regression and vary $k$ instead
        of $\lambda$. Only try 3 values for $k$ ($1,5,$ and $10$) and the $\ell_2$
        norm as your metric. Plot and report the same results as part (b).
\end{enumerate}

\textbf{3} (\textbf{Murphy 2.11}) Derive the normalization constant ($Z$) for a one dimensional
zero-mean Gaussian
\[
    \PP(x; \sigma^2) = \frac{1}{Z}\exp\left(-\frac{x^2}{2\sigma^2}\right)
\]
such that $\PP(x; \sigma^2)$ becomes a valid density.

\textbf{4} (\textbf{Murphy 2.15}) Let $\PP_{emp}(x)$ be the empirical distribution and let
$q(x|\thetab)$ be some model. Show that $\argmin_q \KL(\PP_{emp} || q)$ is obtained by
$q(x) = q(x ; \hat\thetab)$ where $\hat\thetab = \argmax_{\thetab} \;\Lc(q, \Dc)$ is
the maximum likelihood estimate.

\textbf{5} (\textbf{Linear Transformation}) Let $\yy = A\xx + \bb$ be a random vector.
show that expectation is linear:
\[
    \EE[\yy] = \EE[A\xx + \bb] = A\EE[\xx] + \bb.
\]
Also show that
\[
    \cov[\yy] = \cov[A\xx + \bb] = A \cov[\xx] A^\T = A\Sigmab A^\T.
\]

\textbf{6} (\textbf{Murphy 2.16}) Suppose $\theta \sim \text{Beta}(a,b)$ such
that
\[
    \PP(\theta; a,b) = \frac{1}{B(a,b)} \theta^{a-1}(1-\theta)^{b-1} = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \theta^{a-1}(1-\theta)^{b-1}
\]
where $B(a,b) = \Gamma(a)\Gamma(b)/\Gamma(a+b)$ is the Beta function
and $\Gamma(x)$ is the Gamma function.
Derive the mean, mode, and variance of $\theta$.

\textbf{7} (\textbf{Murphy 8.3}) Gradient and Hessian of the log-likelihood for
logistic regression.
\begin{enumerate}[(a)]
    \item Let $\sigma(x) = \frac{1}{1 + e^{-x}}$ be the sigmoid function. Show that
        \[
            \sigma'(x) = \sigma(x)\left[1 - \sigma(x)\right].
        \]
    \item Using the previous result and the chain rule of calculus, derive an
        expression for the gradient of the log likelihood for logistic regression.
    \item The Hessian can be written as $\Hb=\Xb^\T\Sb\Xb$ where $\Sb =
        \diag(\mu_1(1-\mu_1), \dots, \mu_n(1-\mu_n))$. Derive this and show that
        $\Hb \succeq 0$ ($A \succeq 0$ means that $A$ is positive semidefinite).
\end{enumerate}

\textbf{8} (\textbf{Murphy 9}) Show that the multinomial distribution
\[
    \text{Cat}(x|\mub) = \prod_{k=1}^K \mu_k^{x_k}
\]
is in the exponential family and show that the generalized linear model
corresponding to this distribution is the same as multinomial logistic
regression.

\end{document}
